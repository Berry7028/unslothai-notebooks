{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq0eUu-_WhSV"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YclY_l3SWhSY"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUoyUuTbWhSY"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVfMemFuWhSZ"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UxzoCjpqWhSa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!\n",
        "!pip install --upgrade -qqq uv\n",
        "try: import numpy; install_numpy = f\"numpy=={numpy.__version__}\"\n",
        "except: install_numpy = \"numpy\"\n",
        "!uv pip install -qqq \\\n",
        "    \"torch>=2.8.0\" \"triton>=3.4.0\" {install_numpy} \\\n",
        "    \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "    \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "    torchvision bitsandbytes \\\n",
        "    git+https://github.com/huggingface/transformers \\\n",
        "    git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_7bbHKkWhSc"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "We're about to demonstrate the power of the new OpenAI GPT-OSS 20B model through an inference example. For our `MXFP4` version, use this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "2c448e836e684a12a9e3df5e6b13da16",
            "a5984a95e0df4ed6a010ba452accbe6e",
            "36c51389b069473e89254156835e6b8a",
            "063f19a2bdd14916a13f0bb75f795318",
            "c0f8491ffaa641f3abb0721f3f6705a6",
            "f7089198245d4c4aa04ad9bd2c801b6f",
            "3ce3c6d09763498fb200591b8bea3c93",
            "fa3234739e464c5e944e210752d788e4",
            "da68505294534ce585f60c2a9d0d3160",
            "2dd02ae159524877a0a99ae2285dac4c",
            "834cfa3c82874a9e89f9b53179357534"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "7999fda3-bf78-4916-c25a-111bf3893090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.8.3: Fast Gpt_Oss patching. Transformers: 4.56.0.dev0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c448e836e684a12a9e3df5e6b13da16"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
        "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
        "    \"unsloth/gpt-oss-120b\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n",
        "    dtype = None, # None for auto detection\n",
        "    max_seq_length = 4096, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adW8vo6nWhSg"
      },
      "source": [
        "### Reasoning Effort\n",
        "The `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's \"reasoning effort.\" This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.\n",
        "\n",
        "----\n",
        "\n",
        "The `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:\n",
        "\n",
        "* **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.\n",
        "* **Medium**: A balance between performance and speed.\n",
        "* **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sg84JYV9WhSh",
        "outputId": "f55ec88d-326f-460e-c230-73200112c529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-08\n",
            "\n",
            "Reasoning: low\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Solve x^5 + 3x^4 - 10 = 3.<|end|><|start|>assistant<|channel|>analysis<|message|>We need solve equation: x^5 + 3x^4 -10 = 3? Wait rewrite: x^5 + 3x^4 - 10 = 3. So set equal to 3? Actually equation: x^5 + 3x^4 -10 =3. Solve for x. So x^5 +3x^4 -10 =3 => x^5 +3x^4 -13 =0. Solve possible roots? Try integer roots: factors of 13: ¬±1, ¬±13. test x=1: 1+3-13=-9. x=2:32+48-13=67. x=-1: -1+3-13=-11. x=13? huge. No obvious integer root. Maybe approximate? We can look for real root approximate. Let's see function f(x)=x^5+3x^4-13. For large negative, negative large? x=-2: -32+3*16= -32+48=16-13=3>0. Wait f(-2)= (-32)+(48)-13=3. So root between -3 and -2? f(-3): -243+3*81= -243+243=0-13=-13. So f(-3)=-13 <0. f(-2)=3>0. So root between -3 and -2. Do bisection: f(-2.5)=? (-2.5)^5 = -97.656? Actually (-2.5)^5 = -97.65625? Wait compute: 2.5^5= 97.65625. Negative sign => -97.65625. 3*(-2.5)^4 = 3*39.0625=117.1875. Sum: 19.53125-13=6.53125>0. So root > -2.5? Wait f(-2.5)=6.53>0. f(-3)=-13<0. So root between -3 and -2.5. Try -2.75: (-2.75)^5 =? 2.75^5=2.75^2=7.5625; *2.75^3? Let's approximate: 2.75^3=20.796875; times 7.5625? Wait wrong approach. Use calculator mentally: 2.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"low\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Mjq8t1WhSj"
      },
      "source": [
        "Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ugREN9k0WhSk",
        "outputId": "41e7c69d-c98b-4109-a702-b2da32e2c0ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-08\n",
            "\n",
            "Reasoning: medium\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Solve x^5 + 3x^4 - 10 = 3.<|end|><|start|>assistant<|channel|>analysis<|message|>We have the equation x^5 + 3x^4 -10 = 3. Rearranged: x^5 + 3x^4 -10 -3 = 0 => x^5 + 3x^4 -13 = 0. So solving for real x? Likely we need to find solutions. This is a quintic equation. Possibly factorable? Let's try to see if there are rational roots: factors of constant term -13: ¬±1, ¬±13. Check x=1: 1^5 +3*1^4 -13 = 1 +3 -13 = -9. Not zero. x=13: huge. x=-1: -1 +3*1 -13 = -1+3-13=-11. x=13 maybe? Too large. x = 13? Wait maybe the equation is x^5 +3x^4 = 13. So we need solve that. Maybe numeric approximation? Might be integer root 1? no. Try x=2: 32+3*16=32+48=80, minus 13 => 67 ‚â†0. x=1? -9. x ‚âà something? Let's try to find root near 1? Actually f(1)=1+3-13=-9. f(2)=32+48-13=67. So root between 1 and 2. Check 1.1: 1.1^5 ‚âà 1.1^5=1.61051? Actually 1.1^2=1.21, ^3=1.331, ^4=1.4641, ^5=1.61051. 3x^4: 3*1.4641=4.3923. Sum=6.0028-13=-6.9972. So negative. Next 1.3: 1.3^5? 1.3^2=1.69, ^3=2.197, ^4=2.8561, ^5=3.71293. 3x^4: 3*2.8561=8.5683. Sum=12.2812-13=-0.7188. 1.35: 1.35^5? 1.35^2=1.8225; ^3=2.4609; ^4=3.321\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"medium\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skak0iZCWhSl"
      },
      "source": [
        "Lastly we will test it using `reasoning_effort` to `high`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lejQ8LRkWhSl",
        "outputId": "9627691f-edaa-414a-d2ec-a8efba63a6ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-08\n",
            "\n",
            "Reasoning: high\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Solve x^5 + 3x^4 - 10 = 3.<|end|><|start|>assistant<|channel|>analysis<|message|>We need to parse the problem: \"Solve x^5 + 3x^4 - 10 = 3.\" So presumably find real or complex solutions for x? It's a polynomial equation: x^5 + 3x^4 - 10 = 3. Or maybe it's \"x^5 + 3x^4 - 10 = 3\"? Usually they'd set equal to zero: x^5 + 3x^4 - 10 = 3. Wait, but the equation is not in the form f(x)=0: we have a polynomial expression set equal to 3? Typically we would solve for x: find x satisfying x^5 + 3x^4 - 10 = 3.\n",
            "\n",
            "That could be reorganized: x^5 + 3x^4 - 10 - 3 = 0 => x^5 + 3x^4 - 13 = 0.\n",
            "\n",
            "So we need to solve x^5 + 3x^4 - 13 = 0.\n",
            "\n",
            "But maybe the problem wants to find the numerical solution(s). It's a quintic. Typically there's no closed form solution using radicals (Galois theory). But maybe there are rational roots that can be found by rational root theorem.\n",
            "\n",
            "Let's attempt to find rational roots: potential rational roots are ¬±1,¬±13.\n",
            "\n",
            "Let's test: f(1) = 1+3-13= -9. f(-1) = -1+3-13= -11. f(13) huge positive. f(-13) is huge negative. So no obvious small rational root.\n",
            "\n",
            "But maybe it's purposely set so that it has integer root? Let's test x=2: 2^5 + 3*16 -13 = 32 + 48 -13 = 67. Not zero. x=1: -9. So root between 1 and 2? Let's check continuity: at x=1: f= -9, at x=2: f=67 positive; so yes there's a root between 1 and 2. But maybe there is also negative root. f(-1)= -11. f(0)= -13 negative. f(-2)= -32 + 3*16 -13? Wait compute: (-2)^5 = -32. 3*(-2)^4 = 3 * 16 = 48. So f(-2) = -32 + 48 -13 = 3 (actually\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WteSsS-vWhSm"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c448e836e684a12a9e3df5e6b13da16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5984a95e0df4ed6a010ba452accbe6e",
              "IPY_MODEL_36c51389b069473e89254156835e6b8a",
              "IPY_MODEL_063f19a2bdd14916a13f0bb75f795318"
            ],
            "layout": "IPY_MODEL_c0f8491ffaa641f3abb0721f3f6705a6"
          }
        },
        "a5984a95e0df4ed6a010ba452accbe6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7089198245d4c4aa04ad9bd2c801b6f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3ce3c6d09763498fb200591b8bea3c93",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "36c51389b069473e89254156835e6b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa3234739e464c5e944e210752d788e4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da68505294534ce585f60c2a9d0d3160",
            "value": 4
          }
        },
        "063f19a2bdd14916a13f0bb75f795318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dd02ae159524877a0a99ae2285dac4c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_834cfa3c82874a9e89f9b53179357534",
            "value": "‚Äá4/4‚Äá[00:53&lt;00:00,‚Äá11.47s/it]"
          }
        },
        "c0f8491ffaa641f3abb0721f3f6705a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7089198245d4c4aa04ad9bd2c801b6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce3c6d09763498fb200591b8bea3c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa3234739e464c5e944e210752d788e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da68505294534ce585f60c2a9d0d3160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dd02ae159524877a0a99ae2285dac4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834cfa3c82874a9e89f9b53179357534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}